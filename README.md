# ğŸš€ Neural Networks: Zero to Hero

Welcome to my **Neural Networks Training Repo**! This repository captures everything I learn as I dive into the _Zero to Hero_ course, building neural networks, language models, and even GPT from scratch.

---

## ğŸ“š Whatâ€™s Inside?

### 1ï¸âƒ£ **Neural Networks & Backpropagation**

- Build a micro neural network framework: **micrograd**.
- Learn backpropagation step-by-step.
- ğŸ“ [Lecture 1 Notebook](./lectures/lecture1) | ğŸ”— [Micrograd Repo](https://github.com/karpathy/micrograd)

---

### 2ï¸âƒ£ **Bigram Language Model**

- Create a character-level bigram language model.
- Explore model training, sampling, and loss evaluation.
- ğŸ“ [Lecture 2 Notebook](./lectures/lecture2) | ğŸ”— [Makemore Repo](https://github.com/karpathy/makemore)

---

### 3ï¸âƒ£ **Multilayer Perceptrons (MLP)**

- Implement and train an MLP language model.
- Learn hyperparameter tuning, evaluation, and avoiding overfitting.
- ğŸ“ [Lecture 3 Notebook](./lectures/lecture3)

---

### 4ï¸âƒ£ **Advanced MLP: BatchNorm & Gradients**

- Dive into activations, gradients, and BatchNorm.
- Visualize and debug your deep networks.
- ğŸ“ [Lecture 4 Notebook](./lectures/lecture4)

---

### 5ï¸âƒ£ **Backpropagation Deep Dive**

- Manually implement backpropagation for an MLP.
- Gain an intuitive understanding of gradient flow.
- ğŸ“ [Lecture 5 Notebook](./lectures/lecture5) | ğŸ”— [Google Colab Exercise](https://colab.research.google.com/)

---

### 6ï¸âƒ£ **Building a WaveNet**

- Create a hierarchical CNN inspired by WaveNet.
- Explore `torch.nn` and efficient model development.
- ğŸ“ [Lecture 6 Notebook](./lectures/lecture6)

---

### 7ï¸âƒ£ **Building GPT from Scratch**

- Implement a GPT following the "Attention is All You Need" paper.
- Learn the building blocks of transformers.
- ğŸ“ [Lecture 7 Notebook](./lectures/lecture7)

---

### 8ï¸âƒ£ **Tokenizers in GPT**

- Build and understand the GPT Tokenizer.
- Explore how tokenization impacts LLMs.
- ğŸ“ [Lecture 8 Notebook](./lectures/lecture8) | ğŸ”— [minBPE Code](https://github.com/karpathy/minGPT)

---

## ğŸ¯ Goals

- ğŸ“– Master neural networks, language models, and GPT.
- ğŸ’» Gain hands-on experience with PyTorch and deep learning tools.
- ğŸ§  Build intuition and confidence in machine learning.

---

## ğŸš€ How to Use

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/Cluab/neural-networks-training.git
   cd neural-networks-training
   ```

2. **Set Up the Environment:**

   ```bash
   python -m venv venv
   source venv/Scripts/activate
   # Ensure the virtual environment is active before proceeding
   pip install -r requirements.txt
   ```

3. **Explore the Content:**

   - Navigate to the `lectures` folder to access notebooks and code.

4. **Follow the Course:**

   - Watch the [YouTube Playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6) for guided learning.

5. **Engage with Exercises:**
   - Attempt the exercises independently before reviewing the provided solutions.

---

## ğŸ›  Tools & Resources

- **Languages:** Python, PyTorch
- **Guides:**
  - [PyTorch Docs](https://pytorch.org/docs/stable/index.html)
  - [Micrograd Repo](https://github.com/karpathy/micrograd)
  - [Makemore Repo](https://github.com/karpathy/makemore)

---

## ğŸ“ƒ License

This repository follows the MIT License. See the [LICENSE](LICENSE) file for details.
